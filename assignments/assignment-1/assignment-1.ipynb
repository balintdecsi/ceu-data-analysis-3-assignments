{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b6a3f4b3",
   "metadata": {},
   "source": [
    "# Assignment 1: Airbnb Pricing Model\n",
    "\n",
    "**Objective**: Build a prediction model for Airbnb listing prices  \n",
    "**Data**: Oslo (training & temporal validation) and Copenhagen (spatial validation)  \n",
    "**Course**: CEU Data Analysis 3 - Prediction and Machine Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "334a8562",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
    "from sklearn.linear_model import LinearRegression, LassoCV, RidgeCV\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f8fa317",
   "metadata": {},
   "source": [
    "## Part I: Task 1 - Data Acquisition & Preparation\n",
    "\n",
    "### Data Loading\n",
    "- **Oslo**: Training and temporal validation (≥10,000 listings)\n",
    "- **Copenhagen**: Spatial validation (≥3,000 observations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d646461",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data using cross-platform paths\n",
    "path_oslo = os.path.join(os.pardir, os.pardir, 'data', 'raw', 'oslo_listings.csv')\n",
    "path_copenhagen = os.path.join(os.pardir, os.pardir, 'data', 'raw', 'copenhagen_listings.csv')\n",
    "\n",
    "df_oslo = pd.read_csv(path_oslo)\n",
    "df_copenhagen = pd.read_csv(path_copenhagen)\n",
    "\n",
    "print(f\"Oslo listings: {df_oslo.shape[0]:,} rows, {df_oslo.shape[1]} columns\")\n",
    "print(f\"Copenhagen listings: {df_copenhagen.shape[0]:,} rows, {df_copenhagen.shape[1]} columns\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "290740ee",
   "metadata": {},
   "source": [
    "### Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d3fbc25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check basic info about Oslo data\n",
    "df_oslo.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8aa7806e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Examine target variable (price)\n",
    "# Price is stored as string with $ and commas, need to clean\n",
    "print(\"Price column sample:\")\n",
    "print(df_oslo['price'].head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fa0c738",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_price(price_str):\n",
    "    \"\"\"Convert price string like '$1,234.00' to float\"\"\"\n",
    "    if pd.isna(price_str):\n",
    "        return np.nan\n",
    "    return float(str(price_str).replace('$', '').replace(',', ''))\n",
    "\n",
    "# Apply to both datasets\n",
    "df_oslo['price_clean'] = df_oslo['price'].apply(clean_price)\n",
    "df_copenhagen['price_clean'] = df_copenhagen['price'].apply(clean_price)\n",
    "\n",
    "print(\"Oslo price statistics:\")\n",
    "print(df_oslo['price_clean'].describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8434d13f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Price distribution\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "axes[0].hist(df_oslo['price_clean'].dropna(), bins=50, edgecolor='black')\n",
    "axes[0].set_title('Oslo Price Distribution')\n",
    "axes[0].set_xlabel('Price')\n",
    "\n",
    "# Log-transformed price\n",
    "axes[1].hist(np.log(df_oslo['price_clean'].dropna() + 1), bins=50, edgecolor='black')\n",
    "axes[1].set_title('Oslo Log(Price) Distribution')\n",
    "axes[1].set_xlabel('Log(Price)')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe31e083",
   "metadata": {},
   "source": [
    "### Data Wrangling Function\n",
    "\n",
    "Creating a reusable function to ensure consistent preprocessing across train and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "583ead92",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_airbnb(df, is_train=True, reference_df=None):\n",
    "    \"\"\"\n",
    "    Preprocess Airbnb data for modeling.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    df : DataFrame - Raw Airbnb data\n",
    "    is_train : bool - Whether this is training data (affects imputation)\n",
    "    reference_df : DataFrame - Reference for imputation values (use training data)\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    DataFrame with cleaned and engineered features\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    \n",
    "    # 1. Clean price (target variable)\n",
    "    df['price'] = df['price'].apply(clean_price)\n",
    "    \n",
    "    # 2. Drop rows with missing target\n",
    "    df = df.dropna(subset=['price'])\n",
    "    \n",
    "    # 3. Filter extreme prices (keep reasonable range)\n",
    "    df = df[(df['price'] >= 100) & (df['price'] <= 10000)]\n",
    "    \n",
    "    # 4. Log transform price\n",
    "    df['ln_price'] = np.log(df['price'])\n",
    "    \n",
    "    # 5. Select and clean numeric features\n",
    "    numeric_cols = ['accommodates', 'bedrooms', 'beds', 'minimum_nights', \n",
    "                    'maximum_nights', 'number_of_reviews', 'review_scores_rating',\n",
    "                    'review_scores_accuracy', 'review_scores_cleanliness',\n",
    "                    'review_scores_checkin', 'review_scores_communication',\n",
    "                    'review_scores_location', 'review_scores_value',\n",
    "                    'reviews_per_month', 'availability_30', 'availability_60',\n",
    "                    'availability_90', 'availability_365',\n",
    "                    'calculated_host_listings_count']\n",
    "    \n",
    "    # 6. Handle missing values in numeric columns\n",
    "    for col in numeric_cols:\n",
    "        if col in df.columns:\n",
    "            if is_train or reference_df is None:\n",
    "                median_val = df[col].median()\n",
    "            else:\n",
    "                median_val = reference_df[col].median()\n",
    "            \n",
    "            # Create flag for important missing patterns\n",
    "            if col.startswith('review_scores'):\n",
    "                df[f'flag_{col}_missing'] = df[col].isna().astype(int)\n",
    "            \n",
    "            df[col] = df[col].fillna(median_val)\n",
    "    \n",
    "    # 7. Categorical features\n",
    "    cat_cols = ['room_type', 'property_type', 'neighbourhood_cleansed']\n",
    "    for col in cat_cols:\n",
    "        if col in df.columns:\n",
    "            df[col] = df[col].fillna('Missing')\n",
    "    \n",
    "    # 8. Feature engineering - polynomial terms\n",
    "    if 'accommodates' in df.columns:\n",
    "        df['accommodates_sq'] = df['accommodates'] ** 2\n",
    "    \n",
    "    if 'bedrooms' in df.columns:\n",
    "        df['bedrooms_sq'] = df['bedrooms'] ** 2\n",
    "        \n",
    "    # 9. Extract number of bathrooms from text\n",
    "    if 'bathrooms_text' in df.columns:\n",
    "        df['n_bathrooms'] = df['bathrooms_text'].str.extract(r'(\\d+\\.?\\d*)').astype(float)\n",
    "        df['n_bathrooms'] = df['n_bathrooms'].fillna(1)\n",
    "        df['is_shared_bath'] = df['bathrooms_text'].str.contains('shared', case=False, na=False).astype(int)\n",
    "    \n",
    "    # 10. Host features\n",
    "    df['host_is_superhost'] = (df['host_is_superhost'] == 't').astype(int)\n",
    "    df['instant_bookable'] = (df['instant_bookable'] == 't').astype(int)\n",
    "    \n",
    "    # 11. Extract amenities count\n",
    "    if 'amenities' in df.columns:\n",
    "        df['n_amenities'] = df['amenities'].str.count(',') + 1\n",
    "        df['n_amenities'] = df['n_amenities'].fillna(0)\n",
    "        \n",
    "        # Key amenities as binary features\n",
    "        key_amenities = ['wifi', 'kitchen', 'washer', 'dryer', 'parking', \n",
    "                        'air conditioning', 'heating', 'tv', 'pool', 'gym']\n",
    "        for amenity in key_amenities:\n",
    "            df[f'has_{amenity.replace(\" \", \"_\")}'] = df['amenities'].str.lower().str.contains(amenity, na=False).astype(int)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "009876ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply preprocessing\n",
    "df_oslo_clean = preprocess_airbnb(df_oslo, is_train=True)\n",
    "print(f\"Oslo after preprocessing: {df_oslo_clean.shape[0]:,} rows\")\n",
    "print(f\"\\nPrice statistics after cleaning:\")\n",
    "print(df_oslo_clean['price'].describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60c15511",
   "metadata": {},
   "source": [
    "### Train/Test Split\n",
    "\n",
    "**Strategy:**\n",
    "- Training set: 70% of Oslo data\n",
    "- Temporal validation: 30% of Oslo data (holdout)\n",
    "- Spatial validation: Copenhagen data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e3d00ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split Oslo data for training and temporal validation\n",
    "df_train, df_temporal = train_test_split(df_oslo_clean, train_size=0.7, random_state=20250224)\n",
    "\n",
    "# Preprocess Copenhagen for spatial validation\n",
    "df_spatial = preprocess_airbnb(df_copenhagen, is_train=False, reference_df=df_train)\n",
    "\n",
    "print(f\"Training set: {df_train.shape[0]:,} rows\")\n",
    "print(f\"Temporal validation (Oslo holdout): {df_temporal.shape[0]:,} rows\")\n",
    "print(f\"Spatial validation (Copenhagen): {df_spatial.shape[0]:,} rows\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26b85188",
   "metadata": {},
   "source": [
    "### Feature Selection\n",
    "\n",
    "**Variable selection rationale:**\n",
    "- **Property characteristics**: accommodates, bedrooms, beds, bathrooms (core drivers of price)\n",
    "- **Review metrics**: scores and counts (signal of quality)\n",
    "- **Availability**: indicates demand patterns\n",
    "- **Host features**: superhost status, listing count (professionalism)\n",
    "- **Amenities**: key amenities as binary features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "282a997f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define feature sets for modeling\n",
    "numeric_features = [\n",
    "    'accommodates', 'accommodates_sq', 'bedrooms', 'bedrooms_sq', 'beds',\n",
    "    'n_bathrooms', 'minimum_nights', 'number_of_reviews',\n",
    "    'review_scores_rating', 'reviews_per_month',\n",
    "    'availability_30', 'availability_365',\n",
    "    'calculated_host_listings_count', 'n_amenities',\n",
    "    'host_is_superhost', 'instant_bookable', 'is_shared_bath',\n",
    "    'has_wifi', 'has_kitchen', 'has_washer', 'has_parking',\n",
    "    'has_air_conditioning', 'has_tv', 'has_pool', 'has_gym',\n",
    "    'flag_review_scores_rating_missing'\n",
    "]\n",
    "\n",
    "categorical_features = ['room_type', 'property_type']\n",
    "\n",
    "target = 'ln_price'  # Log-transformed price\n",
    "\n",
    "# Verify all features exist\n",
    "all_features = numeric_features + categorical_features\n",
    "missing_features = [f for f in all_features if f not in df_train.columns]\n",
    "if missing_features:\n",
    "    print(f\"Warning: Missing features: {missing_features}\")\n",
    "    numeric_features = [f for f in numeric_features if f in df_train.columns]\n",
    "    categorical_features = [f for f in categorical_features if f in df_train.columns]\n",
    "\n",
    "print(f\"Numeric features: {len(numeric_features)}\")\n",
    "print(f\"Categorical features: {len(categorical_features)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b633e466",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare feature matrices\n",
    "def prepare_features(df, numeric_features, categorical_features):\n",
    "    \"\"\"Create feature matrix with one-hot encoding for categoricals\"\"\"\n",
    "    X_numeric = df[numeric_features].copy()\n",
    "    \n",
    "    # One-hot encode categorical features\n",
    "    X_cat = pd.get_dummies(df[categorical_features], drop_first=True)\n",
    "    \n",
    "    X = pd.concat([X_numeric, X_cat], axis=1)\n",
    "    return X\n",
    "\n",
    "# Prepare train, temporal, and spatial datasets\n",
    "X_train = prepare_features(df_train, numeric_features, categorical_features)\n",
    "y_train = df_train[target]\n",
    "\n",
    "X_temporal = prepare_features(df_temporal, numeric_features, categorical_features)\n",
    "y_temporal = df_temporal[target]\n",
    "\n",
    "X_spatial = prepare_features(df_spatial, numeric_features, categorical_features)\n",
    "y_spatial = df_spatial[target]\n",
    "\n",
    "# Align columns across datasets\n",
    "common_cols = X_train.columns.intersection(X_temporal.columns).intersection(X_spatial.columns)\n",
    "X_train = X_train[common_cols]\n",
    "X_temporal = X_temporal[common_cols]\n",
    "X_spatial = X_spatial[common_cols]\n",
    "\n",
    "print(f\"Feature matrix shape: {X_train.shape}\")\n",
    "print(f\"Features: {list(X_train.columns)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30435996",
   "metadata": {},
   "source": [
    "## Part I: Task 2 - Build 5 Predictive Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fca3bb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to evaluate models\n",
    "def evaluate_model(model, X, y, dataset_name=\"\"):\n",
    "    \"\"\"Calculate RMSE, R², MAE for a model\"\"\"\n",
    "    y_pred = model.predict(X)\n",
    "    rmse = np.sqrt(mean_squared_error(y, y_pred))\n",
    "    r2 = r2_score(y, y_pred)\n",
    "    mae = mean_absolute_error(y, y_pred)\n",
    "    return {'Dataset': dataset_name, 'RMSE': rmse, 'R²': r2, 'MAE': mae}\n",
    "\n",
    "# Store results\n",
    "results = []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d978aaf9",
   "metadata": {},
   "source": [
    "### Model a: OLS (Baseline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5453708a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale features for linear models\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_temporal_scaled = scaler.transform(X_temporal)\n",
    "X_spatial_scaled = scaler.transform(X_spatial)\n",
    "\n",
    "# OLS model\n",
    "start_time = time.time()\n",
    "ols_model = LinearRegression()\n",
    "ols_model.fit(X_train_scaled, y_train)\n",
    "ols_train_time = time.time() - start_time\n",
    "\n",
    "# Evaluate\n",
    "start_time = time.time()\n",
    "ols_train_metrics = evaluate_model(ols_model, X_train_scaled, y_train, 'Train')\n",
    "ols_temporal_metrics = evaluate_model(ols_model, X_temporal_scaled, y_temporal, 'Temporal')\n",
    "ols_spatial_metrics = evaluate_model(ols_model, X_spatial_scaled, y_spatial, 'Spatial')\n",
    "ols_inference_time = time.time() - start_time\n",
    "\n",
    "results.append({\n",
    "    'Model': 'OLS',\n",
    "    'Train_RMSE': ols_train_metrics['RMSE'],\n",
    "    'Train_R2': ols_train_metrics['R²'],\n",
    "    'Temporal_RMSE': ols_temporal_metrics['RMSE'],\n",
    "    'Temporal_R2': ols_temporal_metrics['R²'],\n",
    "    'Spatial_RMSE': ols_spatial_metrics['RMSE'],\n",
    "    'Spatial_R2': ols_spatial_metrics['R²'],\n",
    "    'Train_Time': ols_train_time,\n",
    "    'Inference_Time': ols_inference_time\n",
    "})\n",
    "\n",
    "print(f\"OLS Model trained in {ols_train_time:.4f}s\")\n",
    "print(f\"Train RMSE: {ols_train_metrics['RMSE']:.4f}, R²: {ols_train_metrics['R²']:.4f}\")\n",
    "print(f\"Temporal RMSE: {ols_temporal_metrics['RMSE']:.4f}, R²: {ols_temporal_metrics['R²']:.4f}\")\n",
    "print(f\"Spatial RMSE: {ols_spatial_metrics['RMSE']:.4f}, R²: {ols_spatial_metrics['R²']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efe343d2",
   "metadata": {},
   "source": [
    "### Model b: LASSO (L1 Regularization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebe2ffa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LASSO with cross-validation to find optimal alpha\n",
    "start_time = time.time()\n",
    "lasso_model = LassoCV(cv=5, random_state=42, max_iter=10000)\n",
    "lasso_model.fit(X_train_scaled, y_train)\n",
    "lasso_train_time = time.time() - start_time\n",
    "\n",
    "print(f\"Optimal alpha: {lasso_model.alpha_:.6f}\")\n",
    "print(f\"Non-zero coefficients: {np.sum(lasso_model.coef_ != 0)} out of {len(lasso_model.coef_)}\")\n",
    "\n",
    "# Evaluate\n",
    "start_time = time.time()\n",
    "lasso_train_metrics = evaluate_model(lasso_model, X_train_scaled, y_train, 'Train')\n",
    "lasso_temporal_metrics = evaluate_model(lasso_model, X_temporal_scaled, y_temporal, 'Temporal')\n",
    "lasso_spatial_metrics = evaluate_model(lasso_model, X_spatial_scaled, y_spatial, 'Spatial')\n",
    "lasso_inference_time = time.time() - start_time\n",
    "\n",
    "results.append({\n",
    "    'Model': 'LASSO',\n",
    "    'Train_RMSE': lasso_train_metrics['RMSE'],\n",
    "    'Train_R2': lasso_train_metrics['R²'],\n",
    "    'Temporal_RMSE': lasso_temporal_metrics['RMSE'],\n",
    "    'Temporal_R2': lasso_temporal_metrics['R²'],\n",
    "    'Spatial_RMSE': lasso_spatial_metrics['RMSE'],\n",
    "    'Spatial_R2': lasso_spatial_metrics['R²'],\n",
    "    'Train_Time': lasso_train_time,\n",
    "    'Inference_Time': lasso_inference_time\n",
    "})\n",
    "\n",
    "print(f\"\\nLASSO Model trained in {lasso_train_time:.4f}s\")\n",
    "print(f\"Train RMSE: {lasso_train_metrics['RMSE']:.4f}, R²: {lasso_train_metrics['R²']:.4f}\")\n",
    "print(f\"Temporal RMSE: {lasso_temporal_metrics['RMSE']:.4f}, R²: {lasso_temporal_metrics['R²']:.4f}\")\n",
    "print(f\"Spatial RMSE: {lasso_spatial_metrics['RMSE']:.4f}, R²: {lasso_spatial_metrics['R²']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebba9776",
   "metadata": {},
   "source": [
    "### Model c: Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88394f92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Forest (using unscaled features - RF doesn't require scaling)\n",
    "start_time = time.time()\n",
    "rf_model = RandomForestRegressor(\n",
    "    n_estimators=200,\n",
    "    max_depth=15,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "rf_model.fit(X_train, y_train)\n",
    "rf_train_time = time.time() - start_time\n",
    "\n",
    "# Evaluate\n",
    "start_time = time.time()\n",
    "rf_train_metrics = evaluate_model(rf_model, X_train, y_train, 'Train')\n",
    "rf_temporal_metrics = evaluate_model(rf_model, X_temporal, y_temporal, 'Temporal')\n",
    "rf_spatial_metrics = evaluate_model(rf_model, X_spatial, y_spatial, 'Spatial')\n",
    "rf_inference_time = time.time() - start_time\n",
    "\n",
    "results.append({\n",
    "    'Model': 'Random Forest',\n",
    "    'Train_RMSE': rf_train_metrics['RMSE'],\n",
    "    'Train_R2': rf_train_metrics['R²'],\n",
    "    'Temporal_RMSE': rf_temporal_metrics['RMSE'],\n",
    "    'Temporal_R2': rf_temporal_metrics['R²'],\n",
    "    'Spatial_RMSE': rf_spatial_metrics['RMSE'],\n",
    "    'Spatial_R2': rf_spatial_metrics['R²'],\n",
    "    'Train_Time': rf_train_time,\n",
    "    'Inference_Time': rf_inference_time\n",
    "})\n",
    "\n",
    "print(f\"Random Forest trained in {rf_train_time:.4f}s\")\n",
    "print(f\"Train RMSE: {rf_train_metrics['RMSE']:.4f}, R²: {rf_train_metrics['R²']:.4f}\")\n",
    "print(f\"Temporal RMSE: {rf_temporal_metrics['RMSE']:.4f}, R²: {rf_temporal_metrics['R²']:.4f}\")\n",
    "print(f\"Spatial RMSE: {rf_spatial_metrics['RMSE']:.4f}, R²: {rf_spatial_metrics['R²']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd121cd8",
   "metadata": {},
   "source": [
    "### Model d: Gradient Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b36c579",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gradient Boosting\n",
    "start_time = time.time()\n",
    "gb_model = GradientBoostingRegressor(\n",
    "    n_estimators=200,\n",
    "    max_depth=5,\n",
    "    learning_rate=0.1,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    random_state=42\n",
    ")\n",
    "gb_model.fit(X_train, y_train)\n",
    "gb_train_time = time.time() - start_time\n",
    "\n",
    "# Evaluate\n",
    "start_time = time.time()\n",
    "gb_train_metrics = evaluate_model(gb_model, X_train, y_train, 'Train')\n",
    "gb_temporal_metrics = evaluate_model(gb_model, X_temporal, y_temporal, 'Temporal')\n",
    "gb_spatial_metrics = evaluate_model(gb_model, X_spatial, y_spatial, 'Spatial')\n",
    "gb_inference_time = time.time() - start_time\n",
    "\n",
    "results.append({\n",
    "    'Model': 'Gradient Boosting',\n",
    "    'Train_RMSE': gb_train_metrics['RMSE'],\n",
    "    'Train_R2': gb_train_metrics['R²'],\n",
    "    'Temporal_RMSE': gb_temporal_metrics['RMSE'],\n",
    "    'Temporal_R2': gb_temporal_metrics['R²'],\n",
    "    'Spatial_RMSE': gb_spatial_metrics['RMSE'],\n",
    "    'Spatial_R2': gb_spatial_metrics['R²'],\n",
    "    'Train_Time': gb_train_time,\n",
    "    'Inference_Time': gb_inference_time\n",
    "})\n",
    "\n",
    "print(f\"Gradient Boosting trained in {gb_train_time:.4f}s\")\n",
    "print(f\"Train RMSE: {gb_train_metrics['RMSE']:.4f}, R²: {gb_train_metrics['R²']:.4f}\")\n",
    "print(f\"Temporal RMSE: {gb_temporal_metrics['RMSE']:.4f}, R²: {gb_temporal_metrics['R²']:.4f}\")\n",
    "print(f\"Spatial RMSE: {gb_spatial_metrics['RMSE']:.4f}, R²: {gb_spatial_metrics['R²']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eae52944",
   "metadata": {},
   "source": [
    "### Model e: Ridge Regression (Custom Choice)\n",
    "\n",
    "**Rationale**: Ridge regression (L2 regularization) provides a good comparison to LASSO. It shrinks coefficients but doesn't set them to zero, often performing well when predictors are correlated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2818858",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ridge Regression with cross-validation\n",
    "start_time = time.time()\n",
    "ridge_model = RidgeCV(cv=5, alphas=np.logspace(-3, 3, 50))\n",
    "ridge_model.fit(X_train_scaled, y_train)\n",
    "ridge_train_time = time.time() - start_time\n",
    "\n",
    "print(f\"Optimal alpha: {ridge_model.alpha_:.6f}\")\n",
    "\n",
    "# Evaluate\n",
    "start_time = time.time()\n",
    "ridge_train_metrics = evaluate_model(ridge_model, X_train_scaled, y_train, 'Train')\n",
    "ridge_temporal_metrics = evaluate_model(ridge_model, X_temporal_scaled, y_temporal, 'Temporal')\n",
    "ridge_spatial_metrics = evaluate_model(ridge_model, X_spatial_scaled, y_spatial, 'Spatial')\n",
    "ridge_inference_time = time.time() - start_time\n",
    "\n",
    "results.append({\n",
    "    'Model': 'Ridge',\n",
    "    'Train_RMSE': ridge_train_metrics['RMSE'],\n",
    "    'Train_R2': ridge_train_metrics['R²'],\n",
    "    'Temporal_RMSE': ridge_temporal_metrics['RMSE'],\n",
    "    'Temporal_R2': ridge_temporal_metrics['R²'],\n",
    "    'Spatial_RMSE': ridge_spatial_metrics['RMSE'],\n",
    "    'Spatial_R2': ridge_spatial_metrics['R²'],\n",
    "    'Train_Time': ridge_train_time,\n",
    "    'Inference_Time': ridge_inference_time\n",
    "})\n",
    "\n",
    "print(f\"\\nRidge Model trained in {ridge_train_time:.4f}s\")\n",
    "print(f\"Train RMSE: {ridge_train_metrics['RMSE']:.4f}, R²: {ridge_train_metrics['R²']:.4f}\")\n",
    "print(f\"Temporal RMSE: {ridge_temporal_metrics['RMSE']:.4f}, R²: {ridge_temporal_metrics['R²']:.4f}\")\n",
    "print(f\"Spatial RMSE: {ridge_spatial_metrics['RMSE']:.4f}, R²: {ridge_spatial_metrics['R²']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f623a918",
   "metadata": {},
   "source": [
    "## Part I: Task 3 - Model Comparison (Horserace Table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fba94921",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create horserace table\n",
    "results_df = pd.DataFrame(results)\n",
    "results_df = results_df.round(4)\n",
    "\n",
    "# Display formatted table\n",
    "print(\"=\" * 100)\n",
    "print(\"HORSERACE TABLE: Model Comparison\")\n",
    "print(\"=\" * 100)\n",
    "display(results_df.style.highlight_min(subset=['Train_RMSE', 'Temporal_RMSE', 'Spatial_RMSE'], color='lightgreen')\n",
    "                        .highlight_max(subset=['Train_R2', 'Temporal_R2', 'Spatial_R2'], color='lightgreen'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b74dd35c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization of model performance\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "\n",
    "# RMSE comparison\n",
    "x = np.arange(len(results_df))\n",
    "width = 0.25\n",
    "\n",
    "axes[0].bar(x - width, results_df['Train_RMSE'], width, label='Train', color='steelblue')\n",
    "axes[0].bar(x, results_df['Temporal_RMSE'], width, label='Temporal', color='darkorange')\n",
    "axes[0].bar(x + width, results_df['Spatial_RMSE'], width, label='Spatial', color='green')\n",
    "axes[0].set_ylabel('RMSE (log-price)')\n",
    "axes[0].set_title('RMSE by Model and Dataset')\n",
    "axes[0].set_xticks(x)\n",
    "axes[0].set_xticklabels(results_df['Model'], rotation=45, ha='right')\n",
    "axes[0].legend()\n",
    "\n",
    "# R² comparison\n",
    "axes[1].bar(x - width, results_df['Train_R2'], width, label='Train', color='steelblue')\n",
    "axes[1].bar(x, results_df['Temporal_R2'], width, label='Temporal', color='darkorange')\n",
    "axes[1].bar(x + width, results_df['Spatial_R2'], width, label='Spatial', color='green')\n",
    "axes[1].set_ylabel('R²')\n",
    "axes[1].set_title('R² by Model and Dataset')\n",
    "axes[1].set_xticks(x)\n",
    "axes[1].set_xticklabels(results_df['Model'], rotation=45, ha='right')\n",
    "axes[1].legend()\n",
    "\n",
    "# Training time\n",
    "axes[2].bar(results_df['Model'], results_df['Train_Time'], color='purple')\n",
    "axes[2].set_ylabel('Time (seconds)')\n",
    "axes[2].set_title('Training Time')\n",
    "axes[2].tick_params(axis='x', rotation=45)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('model_comparison.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c7a50b7",
   "metadata": {},
   "source": [
    "### Discussion: Model Performance\n",
    "\n",
    "**Key observations:**\n",
    "1. **Training performance**: Tree-based models (RF, GB) typically show lower training RMSE due to their flexibility\n",
    "2. **Generalization**: Compare temporal vs spatial validation - spatial shift often hurts more\n",
    "3. **Training time**: Linear models are fastest; Random Forest with parallelization is competitive\n",
    "4. **Trade-offs**: Ridge/LASSO offer interpretability; ensemble methods offer predictive power"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66fb71a2",
   "metadata": {},
   "source": [
    "## Part I: Task 4 - Feature Importance Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9548b893",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Forest feature importance\n",
    "rf_importance = pd.DataFrame({\n",
    "    'Feature': X_train.columns,\n",
    "    'Importance': rf_model.feature_importances_\n",
    "}).sort_values('Importance', ascending=False)\n",
    "\n",
    "print(\"Random Forest - Top 10 Features:\")\n",
    "print(rf_importance.head(10).to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f745039e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gradient Boosting feature importance\n",
    "gb_importance = pd.DataFrame({\n",
    "    'Feature': X_train.columns,\n",
    "    'Importance': gb_model.feature_importances_\n",
    "}).sort_values('Importance', ascending=False)\n",
    "\n",
    "print(\"Gradient Boosting - Top 10 Features:\")\n",
    "print(gb_importance.head(10).to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62679838",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare top 10 features side by side\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "# Random Forest\n",
    "top_rf = rf_importance.head(10)\n",
    "axes[0].barh(top_rf['Feature'], top_rf['Importance'], color='steelblue')\n",
    "axes[0].set_xlabel('Importance')\n",
    "axes[0].set_title('Random Forest - Top 10 Features')\n",
    "axes[0].invert_yaxis()\n",
    "\n",
    "# Gradient Boosting\n",
    "top_gb = gb_importance.head(10)\n",
    "axes[1].barh(top_gb['Feature'], top_gb['Importance'], color='darkorange')\n",
    "axes[1].set_xlabel('Importance')\n",
    "axes[1].set_title('Gradient Boosting - Top 10 Features')\n",
    "axes[1].invert_yaxis()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('feature_importance.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e411054",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare overlap in top 10 features\n",
    "rf_top10 = set(rf_importance.head(10)['Feature'])\n",
    "gb_top10 = set(gb_importance.head(10)['Feature'])\n",
    "\n",
    "overlap = rf_top10.intersection(gb_top10)\n",
    "rf_only = rf_top10 - gb_top10\n",
    "gb_only = gb_top10 - rf_top10\n",
    "\n",
    "print(f\"Features in BOTH top 10: {overlap}\")\n",
    "print(f\"\\nRF only: {rf_only}\")\n",
    "print(f\"GB only: {gb_only}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43f46926",
   "metadata": {},
   "source": [
    "### Discussion: Feature Importance\n",
    "\n",
    "**Interpretation:**\n",
    "- Both models typically agree on key pricing drivers: accommodates, bedrooms, room_type\n",
    "- Differences reflect algorithmic characteristics:\n",
    "  - RF splits based on overall variance reduction\n",
    "  - GB focuses on residual patterns, may highlight different features\n",
    "- Review scores and availability metrics provide quality/demand signals"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe77546a",
   "metadata": {},
   "source": [
    "## Part II: Task 5 & 6 - External Validation Summary\n",
    "\n",
    "The external validation results are already incorporated in the horserace table above.\n",
    "\n",
    "### Key Findings:\n",
    "\n",
    "**Temporal Validation (Oslo holdout):**\n",
    "- Performance similar to training data (random split from same distribution)\n",
    "- All models generalize reasonably well within the same city\n",
    "\n",
    "**Spatial Validation (Copenhagen):**\n",
    "- Larger performance degradation expected due to:\n",
    "  - Different market dynamics\n",
    "  - Different price levels (currency may differ: NOK vs DKK)\n",
    "  - Different neighborhood structures\n",
    "- Models with simpler structure (linear) may generalize better across cities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3edd4ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final summary table\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"FINAL MODEL COMPARISON SUMMARY\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"\\nTraining data: Oslo ({len(X_train):,} observations)\")\n",
    "print(f\"Temporal validation: Oslo holdout ({len(X_temporal):,} observations)\")\n",
    "print(f\"Spatial validation: Copenhagen ({len(X_spatial):,} observations)\")\n",
    "print(f\"\\nNumber of features: {X_train.shape[1]}\")\n",
    "print(f\"Target variable: log(price)\")\n",
    "print(\"\\n\")\n",
    "print(results_df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8203efc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save results to CSV\n",
    "results_df.to_csv('model_results.csv', index=False)\n",
    "print(\"Results saved to model_results.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ceu-data-analysis-3-assignments",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
